{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AISA-DucHaba/AI-Solution-Architect/blob/main/AISA_Text_Moderation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WnEac7Fwdgw"
      },
      "source": [
        "# Text Moderation\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "- Let's Rock and Roll\n",
        "\n",
        "- **ROLE** defintion by icon:\n",
        "\n",
        "  - is AI Solution Architect role.\n",
        "  - is AI Scientest role.\n",
        "  - is Devops role.\n",
        "  - is Data Engineer role.\n",
        "  - is AI QA role.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHGKa52qwsp7"
      },
      "source": [
        "## Objective\n",
        "\n",
        "**PRIMARY ROLE:** AI Solution Architect\n",
        "\n",
        "- This NLP (Natural Language Processing) AI demonstration aims to prevent profanity, vulgarity, hate speech, violence, sexism, and other offensive language. It is not an act of censorship, as the final UI (User Interface) will give the reader, but not a young reader, the option to click on a label to read the toxic message.\n",
        "\n",
        "- The goal is to create a safer and more respectful environment for you, your colleages, and your family. This NLP app is 1 of 3 hands-on apps from the [\"AI Solution Architect,\"](https://elvtr.com/course/ai-solution-architect?utm_source=instructor&utm_campaign=AISA&utm_content=linkedin) from ELVTR and Duc Haba.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2GF4K0lcXVp"
      },
      "source": [
        "## Legal:\n",
        "\n",
        "---\n",
        "\n",
        "- This Python Jupyter Notebook is for sharing with **Friends** in the AISA course by ELVTR.\n",
        "\n",
        "- If you are **NOT** my friend, and I see you. In the best spirit of the **science community**, you may read this Notebook, but be aware that I see you.\n",
        "\n",
        "- Copyrights 2023 and 2024: [GNU GENERAL PUBLIC LICENSE 3.0](https://www.gnu.org/licenses/gpl-3.0.en.html#license-text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Tpp017ZnzAkG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current date and time: 2025-10-20 21:25:29\n"
          ]
        }
      ],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "# Get the current date and time\n",
        "now = datetime.now()\n",
        "\n",
        "# Format the date and time\n",
        "current_date_time = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "# Print the current date and time\n",
        "print(\"Current date and time:\", current_date_time)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "BRnCZFMSR0L9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello World! Today is: 2025-10-20 21:25:34.969129\n"
          ]
        }
      ],
      "source": [
        "# prompt: print the time\n",
        "\n",
        "# smoke test\n",
        "import datetime\n",
        "print(f'Hello World! Today is: {datetime.datetime.now()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUogjldqkhPn"
      },
      "source": [
        "# Set Up and Verify\n",
        "\n",
        "---\n",
        "\n",
        "- **PRIMARY ROLE:** DevOps Engineer\n",
        "\n",
        "- This section is setting up your environment and verify the server (or laptop) has the correct library and computing power.\n",
        "\n",
        "- I use the Pluto class often in my coding. I created it as an opensource project.\n",
        "\n",
        "- Github: 'https://github.com/duchaba/pluto_happy\n",
        "\n",
        "- Pluto is **optional** for this project. Pluto has a lot of convience functions that use/write otherwise.\n",
        "\n",
        "- Note: Code cell begin with **\"# prompt:\"** is writen by a GenAI, Copilot or Codey.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFGEfq66P-Hk"
      },
      "source": [
        "## Install required libraries\n",
        "\n",
        "- NOTE : If you run on local laptop or **persistance** platform like, AWS Sagemaker, you need to run pip install just once.\n",
        "\n",
        "- Google Colab is a **non-persistance** platform. Thus you need to install library every time you start up Google Colab.\n",
        "\n",
        "- Hint: The **%%write app.py** is for writing export the code cell for deployment. NOT all code cells are export (because some code are for testing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "pj2BqKWa1Osn"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-2.6.0-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
            "\u001b[?25hCollecting anyio<5,>=3.5.0\n",
            "  Downloading anyio-4.11.0-py3-none-any.whl (109 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.1/109.1 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting distro<2,>=1.7.0\n",
            "  Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
            "Collecting httpx<1,>=0.23.0\n",
            "  Downloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jiter<1,>=0.10.0\n",
            "  Downloading jiter-0.11.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (359 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m359.2/359.2 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting pydantic<3,>=1.9.0\n",
            "  Downloading pydantic-2.12.3-py3-none-any.whl (462 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.4/462.4 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting sniffio\n",
            "  Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
            "Collecting tqdm>4\n",
            "  Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions<5,>=4.11 in ./venv/lib/python3.11/site-packages (from openai) (4.15.0)\n",
            "Collecting idna>=2.8\n",
            "  Downloading idna-3.11-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.0/71.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting certifi\n",
            "  Downloading certifi-2025.10.5-py3-none-any.whl (163 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.3/163.3 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpcore==1.*\n",
            "  Downloading httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11>=0.16\n",
            "  Downloading h11-0.16.0-py3-none-any.whl (37 kB)\n",
            "Collecting annotated-types>=0.6.0\n",
            "  Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
            "Collecting pydantic-core==2.41.4\n",
            "  Downloading pydantic_core-2.41.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspection>=0.4.2\n",
            "  Downloading typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: typing-inspection, tqdm, sniffio, pydantic-core, jiter, idna, h11, distro, certifi, annotated-types, pydantic, httpcore, anyio, httpx, openai\n",
            "Successfully installed annotated-types-0.7.0 anyio-4.11.0 certifi-2025.10.5 distro-1.9.0 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 idna-3.11 jiter-0.11.1 openai-2.6.0 pydantic-2.12.3 pydantic-core-2.41.4 sniffio-1.3.1 tqdm-4.67.1 typing-inspection-0.4.2\n",
            "Collecting gradio\n",
            "  Downloading gradio-5.49.1-py3-none-any.whl (63.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.5/63.5 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting aiofiles<25.0,>=22.0\n",
            "  Downloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in ./venv/lib/python3.11/site-packages (from gradio) (4.11.0)\n",
            "Collecting brotli>=1.1.0\n",
            "  Downloading Brotli-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting fastapi<1.0,>=0.115.2\n",
            "  Downloading fastapi-0.119.1-py3-none-any.whl (108 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.1/108.1 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ffmpy\n",
            "  Downloading ffmpy-0.6.3-py3-none-any.whl (5.5 kB)\n",
            "Collecting gradio-client==1.13.3\n",
            "  Downloading gradio_client-1.13.3-py3-none-any.whl (325 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.4/325.4 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting groovy~=0.1\n",
            "  Downloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in ./venv/lib/python3.11/site-packages (from gradio) (0.28.1)\n",
            "Collecting huggingface-hub<2.0,>=0.33.5\n",
            "  Downloading huggingface_hub-0.35.3-py3-none-any.whl (564 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m564.3/564.3 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting jinja2<4.0\n",
            "  Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.9/134.9 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting markupsafe<4.0,>=2.0\n",
            "  Downloading markupsafe-3.0.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (22 kB)\n",
            "Collecting numpy<3.0,>=1.0\n",
            "  Downloading numpy-2.3.4-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.9/16.9 MB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting orjson~=3.0\n",
            "  Downloading orjson-3.11.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.7/132.7 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in ./venv/lib/python3.11/site-packages (from gradio) (25.0)\n",
            "Collecting pandas<3.0,>=1.0\n",
            "  Downloading pandas-2.3.3-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting pillow<12.0,>=8.0\n",
            "  Downloading pillow-11.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting pydantic<2.12,>=2.0\n",
            "  Downloading pydantic-2.11.10-py3-none-any.whl (444 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m444.8/444.8 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Collecting python-multipart>=0.0.18\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Collecting pyyaml<7.0,>=5.0\n",
            "  Downloading pyyaml-6.0.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (806 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m806.6/806.6 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting ruff>=0.9.3\n",
            "  Downloading ruff-0.14.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m48.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting safehttpx<0.2.0,>=0.1.6\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Collecting semantic-version~=2.0\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting starlette<1.0,>=0.40.0\n",
            "  Downloading starlette-0.48.0-py3-none-any.whl (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.7/73.7 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting tomlkit<0.14.0,>=0.12.0\n",
            "  Downloading tomlkit-0.13.3-py3-none-any.whl (38 kB)\n",
            "Collecting typer<1.0,>=0.12\n",
            "  Downloading typer-0.20.0-py3-none-any.whl (47 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.0/47.0 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions~=4.0 in ./venv/lib/python3.11/site-packages (from gradio) (4.15.0)\n",
            "Collecting uvicorn>=0.14.0\n",
            "  Downloading uvicorn-0.38.0-py3-none-any.whl (68 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.1/68.1 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fsspec\n",
            "  Downloading fsspec-2025.9.0-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.3/199.3 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets<16.0,>=13.0\n",
            "  Downloading websockets-15.0.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (182 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.3/182.3 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna>=2.8 in ./venv/lib/python3.11/site-packages (from anyio<5.0,>=3.0->gradio) (3.11)\n",
            "Requirement already satisfied: sniffio>=1.1 in ./venv/lib/python3.11/site-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in ./venv/lib/python3.11/site-packages (from httpx<1.0,>=0.24.1->gradio) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in ./venv/lib/python3.11/site-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in ./venv/lib/python3.11/site-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
            "Collecting filelock\n",
            "  Downloading filelock-3.20.0-py3-none-any.whl (16 kB)\n",
            "Collecting requests\n",
            "  Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.42.1 in ./venv/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (4.67.1)\n",
            "Collecting hf-xet<2.0.0,>=1.1.3\n",
            "  Downloading hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in ./venv/lib/python3.11/site-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Collecting pytz>=2020.1\n",
            "  Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m509.2/509.2 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tzdata>=2022.7\n",
            "  Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m347.8/347.8 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.6.0 in ./venv/lib/python3.11/site-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Collecting pydantic-core==2.33.2\n",
            "  Downloading pydantic_core-2.33.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-inspection>=0.4.0 in ./venv/lib/python3.11/site-packages (from pydantic<2.12,>=2.0->gradio) (0.4.2)\n",
            "Collecting click>=8.0.0\n",
            "  Downloading click-8.3.0-py3-none-any.whl (107 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.3/107.3 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting shellingham>=1.3.0\n",
            "  Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
            "Collecting rich>=10.11.0\n",
            "  Downloading rich-14.2.0-py3-none-any.whl (243 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.4/243.4 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in ./venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Collecting markdown-it-py>=2.2.0\n",
            "  Downloading markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.3/87.3 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pygments<3.0.0,>=2.13.0 in ./venv/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Collecting charset_normalizer<4,>=2\n",
            "  Downloading charset_normalizer-3.4.4-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (151 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.6/151.6 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting urllib3<3,>=1.21.1\n",
            "  Downloading urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.8/129.8 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting mdurl~=0.1\n",
            "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
            "Installing collected packages: pytz, pydub, brotli, websockets, urllib3, tzdata, tomlkit, shellingham, semantic-version, ruff, pyyaml, python-multipart, pydantic-core, pillow, orjson, numpy, mdurl, markupsafe, hf-xet, groovy, fsspec, filelock, ffmpy, click, charset_normalizer, aiofiles, uvicorn, starlette, requests, pydantic, pandas, markdown-it-py, jinja2, safehttpx, rich, huggingface-hub, fastapi, typer, gradio-client, gradio\n",
            "  Attempting uninstall: pydantic-core\n",
            "    Found existing installation: pydantic_core 2.41.4\n",
            "    Uninstalling pydantic_core-2.41.4:\n",
            "      Successfully uninstalled pydantic_core-2.41.4\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.12.3\n",
            "    Uninstalling pydantic-2.12.3:\n",
            "      Successfully uninstalled pydantic-2.12.3\n",
            "Successfully installed aiofiles-24.1.0 brotli-1.1.0 charset_normalizer-3.4.4 click-8.3.0 fastapi-0.119.1 ffmpy-0.6.3 filelock-3.20.0 fsspec-2025.9.0 gradio-5.49.1 gradio-client-1.13.3 groovy-0.1.2 hf-xet-1.1.10 huggingface-hub-0.35.3 jinja2-3.1.6 markdown-it-py-4.0.0 markupsafe-3.0.3 mdurl-0.1.2 numpy-2.3.4 orjson-3.11.3 pandas-2.3.3 pillow-11.3.0 pydantic-2.11.10 pydantic-core-2.33.2 pydub-0.25.1 python-multipart-0.0.20 pytz-2025.2 pyyaml-6.0.3 requests-2.32.5 rich-14.2.0 ruff-0.14.1 safehttpx-0.1.6 semantic-version-2.10.0 shellingham-1.5.4 starlette-0.48.0 tomlkit-0.13.3 typer-0.20.0 tzdata-2025.2 urllib3-2.5.0 uvicorn-0.38.0 websockets-15.0.1\n",
            "Requirement already satisfied: huggingface_hub in ./venv/lib/python3.11/site-packages (0.35.3)\n",
            "Requirement already satisfied: filelock in ./venv/lib/python3.11/site-packages (from huggingface_hub) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in ./venv/lib/python3.11/site-packages (from huggingface_hub) (2025.9.0)\n",
            "Requirement already satisfied: packaging>=20.9 in ./venv/lib/python3.11/site-packages (from huggingface_hub) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in ./venv/lib/python3.11/site-packages (from huggingface_hub) (6.0.3)\n",
            "Requirement already satisfied: requests in ./venv/lib/python3.11/site-packages (from huggingface_hub) (2.32.5)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in ./venv/lib/python3.11/site-packages (from huggingface_hub) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./venv/lib/python3.11/site-packages (from huggingface_hub) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./venv/lib/python3.11/site-packages (from huggingface_hub) (1.1.10)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in ./venv/lib/python3.11/site-packages (from requests->huggingface_hub) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.11/site-packages (from requests->huggingface_hub) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.11/site-packages (from requests->huggingface_hub) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.11/site-packages (from requests->huggingface_hub) (2025.10.5)\n"
          ]
        }
      ],
      "source": [
        "# use capture to hide the long output\n",
        "#%%capture log_pip_install_openai\n",
        "\n",
        "!pip install openai\n",
        "!pip install gradio\n",
        "!pip install huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U2MogmXySWhj"
      },
      "outputs": [],
      "source": [
        "# If has error, uncomment and print out the log file\n",
        "# print(log_pip_install_openai)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "wNsvTFuhJxeX"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/tbennett/Code/AISolutionArchitect/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "# %%write app.py\n",
        "# prompt: import openai, huggingface client and gradio\n",
        "\n",
        "import openai\n",
        "import gradio\n",
        "import huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "sRERRYZpS1Ta"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "openai version: 2.6.0\n",
            "gradio version: 5.49.1\n",
            "huggingface_hub version: 0.35.3\n"
          ]
        }
      ],
      "source": [
        "# prompt: print out version of openai, gradio and huggingface_hub\n",
        "\n",
        "print(f\"openai version: {openai.__version__}\")\n",
        "print(f\"gradio version: {gradio.__version__}\")\n",
        "print(f\"huggingface_hub version: {huggingface_hub.__version__}\")\n",
        "\n",
        "# expected values:\n",
        "# openai version: 1.35.14\n",
        "# gradio version: 4.38.1\n",
        "# huggingface_hub version: 0.23.4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6R1i47NZTG2J"
      },
      "source": [
        "- Note: I am using/testing with these version.\n",
        "\n",
        "  - openai version: 1.35.14\n",
        "  - gradio version: 4.38.1\n",
        "  - huggingface_hub version: 0.23.4\n",
        "\n",
        "- You don't have to use the exact version but cognizant of them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQ4DvPjRT3-I"
      },
      "source": [
        "## Install Pluto (Optional)\n",
        "\n",
        "- **PRIMARY ROLE:** AI Solution Arch.\n",
        "\n",
        "- NOTE : Once again **pluto class is optional**. You are free to use other lib or none at all.\n",
        "\n",
        "- It is a base class that everyone in the team should use in their Jupyter Notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "r2DeeoKBTvRz"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "git version 2.39.5\n"
          ]
        }
      ],
      "source": [
        "# prompt: print git version\n",
        "\n",
        "!git --version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OxFOHNCURTH"
      },
      "source": [
        "- Note: I am using this git version:\n",
        "  - git version 2.34.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "KhXCeZTiUihl"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;31mE: \u001b[0mCould not open lock file /var/lib/dpkg/lock-frontend - open (13: Permission denied)\u001b[0m\n",
            "\u001b[1;31mE: \u001b[0mUnable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), are you root?\u001b[0m\n",
            "git: 'lfs' is not a git command. See 'git --help'.\n",
            "\n",
            "The most similar command is\n",
            "\tlog\n",
            "git: 'lfs' is not a git command. See 'git --help'.\n",
            "\n",
            "The most similar command is\n",
            "\tlog\n"
          ]
        }
      ],
      "source": [
        "# prompt: install lfs and track large file *.pkl\n",
        "\n",
        "# note this optional for git to upload/push large file like the inference engine, *.pkl file\n",
        "!apt -y install git-lfs\n",
        "!git lfs install\n",
        "!git lfs track \"*.pkl\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "lopo_0f6nUxH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'pluto_happy'...\n",
            "remote: Enumerating objects: 230, done.\u001b[K\n",
            "remote: Counting objects: 100% (65/65), done.\u001b[K\n",
            "remote: Compressing objects: 100% (46/46), done.\u001b[K\n",
            "remote: Total 230 (delta 36), reused 40 (delta 16), pack-reused 165 (from 1)\u001b[K\n",
            "Receiving objects: 100% (230/230), 37.06 MiB | 46.28 MiB/s, done.\n",
            "Resolving deltas: 100% (130/130), done.\n"
          ]
        }
      ],
      "source": [
        "# prompt: clone https://github.com/duchaba/pluto_happy\n",
        "\n",
        "fname = 'https://github.com/duchaba/pluto_happy'\n",
        "!git clone {fname}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "2Tn2IezlVhEY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 5660\n",
            "drwxrwxrwx 4 tbennett admin    4096 Oct 20 21:31 .\n",
            "drwxrwxrwx 5 tbennett admin    4096 Oct 20 21:31 ..\n",
            "-rwxrwxrwx 1 tbennett admin   15790 Oct 20 21:31 app_pluto_deploy.py\n",
            "-rwxrwxrwx 1 tbennett admin    6148 Oct 20 21:31 .DS_Store\n",
            "-rwxrwxrwx 1 tbennett admin 5578756 Oct 20 21:31 fastai_2023_image_classification_foxy.ipynb\n",
            "drwxrwxrwx 8 tbennett admin    4096 Oct 20 21:31 .git\n",
            "-rwxrwxrwx 1 tbennett admin    1065 Oct 20 21:31 LICENSE\n",
            "drwxrwxrwx 2 tbennett admin    4096 Oct 20 21:31 notebooks\n",
            "-rwxrwxrwx 1 tbennett admin   17368 Oct 20 21:31 pluto_deploy.py\n",
            "-rwxrwxrwx 1 tbennett admin   67922 Oct 20 21:31 pluto_foxy.py\n",
            "-rwxrwxrwx 1 tbennett admin   30815 Oct 20 21:31 pluto_huggingface.py\n",
            "-rwxrwxrwx 1 tbennett admin   23406 Oct 20 21:31 pluto.py\n",
            "-rwxrwxrwx 1 tbennett admin     573 Oct 20 21:31 README.md\n",
            "-rwxrwxrwx 1 tbennett admin      34 Oct 20 21:31 requirements_deploy.txt\n",
            "-rwxrwxrwx 1 tbennett admin     153 Oct 20 21:31 requirements_foxy.txt\n",
            "-rwxrwxrwx 1 tbennett admin     100 Oct 20 21:31 requirements_huggingface.txt\n",
            "-rwxrwxrwx 1 tbennett admin      53 Oct 20 21:31 requirements.txt\n"
          ]
        }
      ],
      "source": [
        "# prompt: list the content of pluto_happy directory\n",
        "\n",
        "!ls -la pluto_happy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "WzkHWp-CnU8D"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting cryptography\n",
            "  Using cached cryptography-46.0.3-cp311-abi3-manylinux_2_34_x86_64.whl (4.5 MB)\n",
            "Requirement already satisfied: huggingface_hub in ./venv/lib/python3.11/site-packages (from -r pluto_happy/requirements.txt (line 2)) (0.35.3)\n",
            "Collecting pynvml\n",
            "  Using cached pynvml-13.0.1-py3-none-any.whl (28 kB)\n",
            "Collecting py-cpuinfo\n",
            "  Using cached py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
            "Collecting flopth\n",
            "  Using cached flopth-0.1.6-py3-none-any.whl (11 kB)\n",
            "Collecting cffi>=2.0.0\n",
            "  Using cached cffi-2.0.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (215 kB)\n",
            "Requirement already satisfied: filelock in ./venv/lib/python3.11/site-packages (from huggingface_hub->-r pluto_happy/requirements.txt (line 2)) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in ./venv/lib/python3.11/site-packages (from huggingface_hub->-r pluto_happy/requirements.txt (line 2)) (2025.9.0)\n",
            "Requirement already satisfied: packaging>=20.9 in ./venv/lib/python3.11/site-packages (from huggingface_hub->-r pluto_happy/requirements.txt (line 2)) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in ./venv/lib/python3.11/site-packages (from huggingface_hub->-r pluto_happy/requirements.txt (line 2)) (6.0.3)\n",
            "Requirement already satisfied: requests in ./venv/lib/python3.11/site-packages (from huggingface_hub->-r pluto_happy/requirements.txt (line 2)) (2.32.5)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in ./venv/lib/python3.11/site-packages (from huggingface_hub->-r pluto_happy/requirements.txt (line 2)) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./venv/lib/python3.11/site-packages (from huggingface_hub->-r pluto_happy/requirements.txt (line 2)) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./venv/lib/python3.11/site-packages (from huggingface_hub->-r pluto_happy/requirements.txt (line 2)) (1.1.10)\n",
            "Collecting nvidia-ml-py>=12.0.0\n",
            "  Using cached nvidia_ml_py-13.580.82-py3-none-any.whl (49 kB)\n",
            "Requirement already satisfied: numpy>=1.20 in ./venv/lib/python3.11/site-packages (from flopth->-r pluto_happy/requirements.txt (line 5)) (2.3.4)\n",
            "Collecting tabulate>=0.9.0\n",
            "  Using cached tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
            "Collecting torch>=1.6.0\n",
            "  Using cached torch-2.9.0-cp311-cp311-manylinux_2_28_x86_64.whl (899.8 MB)\n",
            "Collecting torchvision>=0.20.1\n",
            "  Using cached torchvision-0.24.0-cp311-cp311-manylinux_2_28_x86_64.whl (8.0 MB)\n",
            "Collecting pycparser\n",
            "  Using cached pycparser-2.23-py3-none-any.whl (118 kB)\n",
            "Collecting sympy>=1.13.3\n",
            "  Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
            "Collecting networkx>=2.5.1\n",
            "  Using cached networkx-3.5-py3-none-any.whl (2.0 MB)\n",
            "Requirement already satisfied: jinja2 in ./venv/lib/python3.11/site-packages (from torch>=1.6.0->flopth->-r pluto_happy/requirements.txt (line 5)) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.8.93\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.8.90\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.8.90\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
            "Collecting nvidia-cudnn-cu12==9.10.2.21\n",
            "  Using cached nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n",
            "Collecting nvidia-cublas-cu12==12.8.4.1\n",
            "  Using cached nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
            "Collecting nvidia-cufft-cu12==11.3.3.83\n",
            "  Using cached nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.9.90\n",
            "  Using cached nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.7.3.90\n",
            "  Using cached nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.5.8.93\n",
            "  Using cached nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
            "Collecting nvidia-cusparselt-cu12==0.7.1\n",
            "  Using cached nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)\n",
            "Collecting nvidia-nccl-cu12==2.27.5\n",
            "  Downloading nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.3 MB)\n",
            "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m314.1/322.3 MB\u001b[0m \u001b[31m70.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: Exception:\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/tbennett/Code/AISolutionArchitect/venv/lib/python3.11/site-packages/pip/_vendor/urllib3/response.py\", line 438, in _error_catcher\n",
            "    yield\n",
            "  File \"/home/tbennett/Code/AISolutionArchitect/venv/lib/python3.11/site-packages/pip/_vendor/urllib3/response.py\", line 561, in read\n",
            "    data = self._fp_read(amt) if not fp_closed else b\"\"\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/tbennett/Code/AISolutionArchitect/venv/lib/python3.11/site-packages/pip/_vendor/urllib3/response.py\", line 527, in _fp_read\n",
            "    return self._fp.read(amt) if amt is not None else self._fp.read()\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/tbennett/Code/AISolutionArchitect/venv/lib/python3.11/site-packages/pip/_vendor/cachecontrol/filewrapper.py\", line 94, in read\n",
            "    self.__buf.write(data)\n",
            "  File \"/usr/lib/python3.11/tempfile.py\", line 638, in func_wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "OSError: [Errno 28] No space left on device\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/tbennett/Code/AISolutionArchitect/venv/lib/python3.11/site-packages/pip/_internal/cli/base_command.py\", line 160, in exc_logging_wrapper\n",
            "    status = run_func(*args)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/home/tbennett/Code/AISolutionArchitect/venv/lib/python3.11/site-packages/pip/_internal/cli/req_command.py\", line 247, in wrapper\n",
            "    return func(self, options, args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/tbennett/Code/AISolutionArchitect/venv/lib/python3.11/site-packages/pip/_internal/commands/install.py\", line 419, in run\n",
            "    requirement_set = resolver.resolve(\n",
            "                      ^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/tbennett/Code/AISolutionArchitect/venv/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/resolver.py\", line 92, in resolve\n",
            "    result = self._result = resolver.resolve(\n",
            "                            ^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/tbennett/Code/AISolutionArchitect/venv/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py\", line 481, in resolve\n",
            "    state = resolution.resolve(requirements, max_rounds=max_rounds)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/tbennett/Code/AISolutionArchitect/venv/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py\", line 373, in resolve\n",
            "    failure_causes = self._attempt_to_pin_criterion(name)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/tbennett/Code/AISolutionArchitect/venv/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py\", line 213, in _attempt_to_pin_criterion\n",
            "    criteria = self._get_updated_criteria(candidate)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/tbennett/Code/AISolutionArchitect/venv/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py\", line 204, in _get_updated_criteria\n",
            "    self._add_to_criteria(criteria, requirement, parent=candidate)\n",
            "  File \"/home/tbennett/Code/AISolutionArchitect/venv/lib/python3.11/site-packages/pip/_vendor/resolvelib/resolvers.py\", line 172, in _add_to_criteria\n",
            "    if not criterion.candidates:\n",
            "  File \"/home/tbennett/Code/AISolutionArchitect/venv/lib/python3.11/site-packages/pip/_vendor/resolvelib/structs.py\", line 151, in __bool__\n",
            "    return bool(self._sequence)\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/tbennett/Code/AISolutionArchitect/venv/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py\", line 155, in __bool__\n",
            "    return any(self)\n",
            "           ^^^^^^^^^\n",
            "  File \"/home/tbennett/Code/AISolutionArchitect/venv/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py\", line 143, in <genexpr>\n",
            "    return (c for c in iterator if id(c) not in self._incompatible_ids)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/tbennett/Code/AISolutionArchitect/venv/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/found_candidates.py\", line 47, in _iter_built\n",
            "    candidate = func()\n",
            "                ^^^^^^\n",
            "  File \"/home/tbennett/Code/AISolutionArchitect/venv/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/factory.py\", line 206, in _make_candidate_from_link\n",
            "    self._link_candidate_cache[link] = LinkCandidate(\n",
            "                                       ^^^^^^^^^^^^^^\n",
            "  File \"/home/tbennett/Code/AISolutionArchitect/venv/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 297, in __init__\n",
            "    super().__init__(\n",
            "  File \"/home/tbennett/Code/AISolutionArchitect/venv/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 162, in __init__\n",
            "    self.dist = self._prepare()\n",
            "                ^^^^^^^^^^^^^^^\n",
            "  File \"/home/tbennett/Code/AISolutionArchitect/venv/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 231, in _prepare\n",
            "    dist = self._prepare_distribution()\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/tbennett/Code/AISolutionArchitect/venv/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py\", line 308, in _prepare_distribution\n",
            "    return preparer.prepare_linked_requirement(self._ireq, parallel_builds=True)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/tbennett/Code/AISolutionArchitect/venv/lib/python3.11/site-packages/pip/_internal/operations/prepare.py\", line 491, in prepare_linked_requirement\n",
            "    return self._prepare_linked_requirement(req, parallel_builds)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/tbennett/Code/AISolutionArchitect/venv/lib/python3.11/site-packages/pip/_internal/operations/prepare.py\", line 536, in _prepare_linked_requirement\n",
            "    local_file = unpack_url(\n",
            "                 ^^^^^^^^^^^\n",
            "  File \"/home/tbennett/Code/AISolutionArchitect/venv/lib/python3.11/site-packages/pip/_internal/operations/prepare.py\", line 166, in unpack_url\n",
            "    file = get_http_url(\n",
            "           ^^^^^^^^^^^^^\n",
            "  File \"/home/tbennett/Code/AISolutionArchitect/venv/lib/python3.11/site-packages/pip/_internal/operations/prepare.py\", line 107, in get_http_url\n",
            "    from_path, content_type = download(link, temp_dir.path)\n",
            "                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/tbennett/Code/AISolutionArchitect/venv/lib/python3.11/site-packages/pip/_internal/network/download.py\", line 147, in __call__\n",
            "    for chunk in chunks:\n",
            "  File \"/home/tbennett/Code/AISolutionArchitect/venv/lib/python3.11/site-packages/pip/_internal/cli/progress_bars.py\", line 53, in _rich_progress_bar\n",
            "    for chunk in iterable:\n",
            "  File \"/home/tbennett/Code/AISolutionArchitect/venv/lib/python3.11/site-packages/pip/_internal/network/utils.py\", line 63, in response_chunks\n",
            "    for chunk in response.raw.stream(\n",
            "  File \"/home/tbennett/Code/AISolutionArchitect/venv/lib/python3.11/site-packages/pip/_vendor/urllib3/response.py\", line 622, in stream\n",
            "    data = self.read(amt=amt, decode_content=decode_content)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/tbennett/Code/AISolutionArchitect/venv/lib/python3.11/site-packages/pip/_vendor/urllib3/response.py\", line 560, in read\n",
            "    with self._error_catcher():\n",
            "  File \"/usr/lib/python3.11/contextlib.py\", line 155, in __exit__\n",
            "    self.gen.throw(typ, value, traceback)\n",
            "  File \"/home/tbennett/Code/AISolutionArchitect/venv/lib/python3.11/site-packages/pip/_vendor/urllib3/response.py\", line 455, in _error_catcher\n",
            "    raise ProtocolError(\"Connection broken: %r\" % e, e)\n",
            "pip._vendor.urllib3.exceptions.ProtocolError: (\"Connection broken: OSError(28, 'No space left on device')\", OSError(28, 'No space left on device'))\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# prompt: pip install requriements.txt\n",
        "\n",
        "#%%capture log_pip_install\n",
        "fname = 'pluto_happy/requirements.txt'\n",
        "!pip install -r {fname}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EwaQnDnBxa4F"
      },
      "outputs": [],
      "source": [
        "# print the log file is failed\n",
        "# log_pip_install.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "3ZeBBZZXojkF"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'torch'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Code/AISolutionArchitect/pluto_happy/pluto.py:3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# [BEGIN OF pluto_happy]\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m## required lib, required \"pip install\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcryptography\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcryptography\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfernet\u001b[39;00m\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'torch'"
          ]
        }
      ],
      "source": [
        "# prompt: run the pluto_happy/pluto.py\n",
        "\n",
        "fname = 'pluto_happy/pluto.py'\n",
        "%run {fname}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yt_Mqx_JWopf"
      },
      "outputs": [],
      "source": [
        "# %%write -a app.py\n",
        "# prompt: create a new class Pluto_Happy and name it monty\n",
        "\n",
        "monty = Pluto_Happy('Monty, Monty Said!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KxuRP5nbojqq"
      },
      "outputs": [],
      "source": [
        "# not_write -a app.py\n",
        "# prompt: None.\n",
        "\n",
        "# print out my environments\n",
        "monty.fname_requirements = 'pluto_happy/requirements.txt'\n",
        "monty.print_info_self()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlsCH9vMYRS3"
      },
      "source": [
        "- NOTE: Once again, the use of **pluto.print_info_self()** function is for convience. You can write Python code to do the same."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqpkzBwRbaPF"
      },
      "outputs": [],
      "source": [
        "# prompt: print all monty functions doc\n",
        "\n",
        "help(monty)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAg9QSv6hMQm"
      },
      "source": [
        "- The documentation is at: https://platform.openai.com/docs/api-reference\n",
        "\n",
        "- https://github.com/openai/openai-python\n",
        "\n",
        "- The Notebook set to GPU and High RAM, but you do not need them to run. It maybe slow without GPU and a lot of RAM, but it will fine.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYTj6YmpApba"
      },
      "source": [
        "# Access to LLM model: STEP 1\n",
        "---\n",
        "\n",
        "- **PRIMARY ROLE:** AI Solution Architect\n",
        "\n",
        "- NOTES:  STOP, define your set of keys before continue\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLElZNzgMi8l"
      },
      "source": [
        "## Define YOUR Keys: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J3NPJ7l7nlPF"
      },
      "outputs": [],
      "source": [
        "# # update and uncomment with your key\n",
        "# os.environ['openai_key'] = 'sk-...'\n",
        "# os.environ['huggingface_key'] = 'hf_....'\n",
        "# os.environ['kaggle_key'] = 'daabc...'\n",
        "# os.environ['github_key'] = 'ghp_...'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T3g9l-KGypTA"
      },
      "outputs": [],
      "source": [
        "# Prompt: None:\n",
        "\n",
        "# YOUR KEY GOES HERE...\n",
        "import os\n",
        "import os\n",
        "# os.environ['crypt_key'] = ''\n",
        "# os.environ['openai_key'] = ''\n",
        "# os.environ['github_key'] = ''\n",
        "# os.environ['huggingface_key'] = ''\n",
        "# os.environ['kaggle_key'] = ''\n",
        "# os.environ['google_key'] = ''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2BgHE64HMCqi"
      },
      "outputs": [],
      "source": [
        "# %%write app.py\n",
        "# Prompt: None\n",
        "# replace the \"getenv()\" with your key string\n",
        "\n",
        "import os\n",
        "monty._openai_key=os.getenv('openai_key')\n",
        "monty._github_key=os.getenv('github_key')\n",
        "monty._huggingface_key=os.getenv('huggingface_key')\n",
        "monty._kaggle_key=os.getenv('kaggle_key')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgCpuPolnmPY"
      },
      "source": [
        "#  Access LLM: STEP 2\n",
        "\n",
        "----\n",
        "\n",
        "- **PRIMARY ROLE:** AI Scientist\n",
        "\n",
        "- Smoke test to see if we can access to the chosen model and run it.\n",
        "\n",
        "- We choose OpenAI Moderate model.\n",
        "  - https://platform.openai.com/docs/guides/moderation\n",
        "\n",
        "- update to OpenAI 1.x API\n",
        "  - first create a class openai.OpenAI (a client)\n",
        "  - then do the task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSH08CSL96fN"
      },
      "source": [
        "## POC - OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6dijfYPyiq97"
      },
      "outputs": [],
      "source": [
        "# prompt: create an ai client with openai\n",
        "\n",
        "import openai\n",
        "ai_client = openai.OpenAI(api_key=monty._openai_key)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dPDm1by88qEb"
      },
      "outputs": [],
      "source": [
        "# help(ai_client.moderations.create)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8fLs18n_7AHq"
      },
      "outputs": [],
      "source": [
        "# prompt: None (use same code in the openAI doc site)\n",
        "\n",
        "p = \"I am but a sheep who is lost in the wood.\"\n",
        "tmodel = \"text-moderation-latest\"\n",
        "resp = ai_client.moderations.create(input=p, model=tmodel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RsQIEmNMjbBL"
      },
      "outputs": [],
      "source": [
        "# prompt: display resp content\n",
        "\n",
        "resp.results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sb-tGagMkCwV"
      },
      "outputs": [],
      "source": [
        "# prompt: parse resp variable\n",
        "\n",
        "# parse the response\n",
        "for idx, cat in enumerate(resp.results[0].categories):\n",
        "  print(f\"{idx} category: {cat}\")\n",
        "print('SCORE:')\n",
        "for idx, cat in enumerate(resp.results[0].category_scores):\n",
        "  print(f\"{idx} category: {cat}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jNO1cAXNZKr"
      },
      "source": [
        "## POC - Done \n",
        "---\n",
        "\n",
        "- You just prove the heart of the LLM engine is working.\n",
        "\n",
        "- Technically, you can confidently say the project is \"viable.\"\n",
        "\n",
        "- The crucial part POC is done."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcZ0RUl35fY_"
      },
      "source": [
        "###  AI Scientist Walk About on OpenAI (Optional)\n",
        "\n",
        "\n",
        "- AI Scientist to explore futher on OpenAI models.\n",
        "\n",
        "- Doc at: https://platform.openai.com/docs/api-reference\n",
        "\n",
        "- Check out Assistants\n",
        "\n",
        "- Use Dall-e to draw image\n",
        "\n",
        "I delete code so it up to you.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HW7lY4ET7zb"
      },
      "source": [
        "#  Fetch Dataset from Kaggle: STEP 3\n",
        "\n",
        "- **PRIMARY ROLE:** Data Engineer\n",
        "\n",
        "- Dataset on kaggle: https://www.kaggle.com/datasets/get2jawa/toxic-comments-train\n",
        "\n",
        "- Goals are:\n",
        "  - download\n",
        "  - import to Pandas Dataframe\n",
        "  - clean\n",
        "  - augment\n",
        "  - inspect\n",
        "  - report on biases\n",
        "  - **AND** whatever you need to do to feel confortable with the datasest."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MpCcJs0WfigG"
      },
      "outputs": [],
      "source": [
        "# prompt install opendatasets\n",
        "!pip install opendatasets\n",
        "import opendatasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AndgQlqQuRVC"
      },
      "source": [
        "## Download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SysYu5XWTADD"
      },
      "outputs": [],
      "source": [
        "# prompt: Write function with inline documentation to download dataset from Kaggle website using opendatasets lib.\n",
        "\n",
        "# I add line @add_method for put the function to pluto (Note: this is optional)\n",
        "@add_method(Pluto_Happy)\n",
        "def fetch_kaggle_dataset(self,dataset_name, path_to_save):\n",
        "\n",
        "  \"\"\"\n",
        "  Downloads a dataset from Kaggle website using opendatasets library.\n",
        "\n",
        "  Args:\n",
        "    dataset_name: (str) The name of the dataset to download.\n",
        "    path_to_save: (str) The path where the dataset will be saved.\n",
        "\n",
        "  Returns:\n",
        "    None\n",
        "  \"\"\"\n",
        "\n",
        "  try:\n",
        "    # Check if the dataset already exists\n",
        "    if os.path.exists(path_to_save):\n",
        "      print(f'Dataset {dataset_name} already exists.')\n",
        "      return\n",
        "\n",
        "    # Download the dataset\n",
        "    print(f'Downloading dataset {dataset_name}...')\n",
        "    opendatasets.download(dataset_name, path_to_save)\n",
        "    print(f'Dataset {dataset_name} downloaded successfully.')\n",
        "\n",
        "  except Exception as e:\n",
        "    print(f'Error downloading dataset {dataset_name}: {e}')\n",
        "  return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGbgPXyTtckU"
      },
      "source": [
        "- Note: You need your kaggle username and access token\n",
        "- Please \"join\" the Jigsaw compitition.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQQXCl-ysrwI"
      },
      "outputs": [],
      "source": [
        "# prompt: use monty.fetch_kaggle_dataset to download https://www.kaggle.com/competitions/jigsaw-toxic-severity-rating\n",
        "\n",
        "fname = 'https://www.kaggle.com/competitions/jigsaw-toxic-severity-rating'\n",
        "monty.fetch_kaggle_dataset(fname,'kaggle')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EqliK30EOm4t"
      },
      "outputs": [],
      "source": [
        "# prompt: print today date and time\n",
        "\n",
        "print(datetime.datetime.now())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XWgb7GduiGhq"
      },
      "outputs": [],
      "source": [
        "# prompt: list the data in kaggle/jigsaw-toxic-severity-rating\n",
        "!ls -la kaggle/jigsaw-toxic-severity-rating"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6izTs_92uXvG"
      },
      "source": [
        "## Import to Pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zhqTyIFVT6sT"
      },
      "outputs": [],
      "source": [
        "# prompt: load fname csv into dataframe\n",
        "\n",
        "import pandas\n",
        "fname = '/content/kaggle/jigsaw-toxic-severity-rating/validation_data.csv'\n",
        "monty.df_toxic_data = pandas.read_csv(fname)\n",
        "monty.df_toxic_data.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APfUKF2Qut8X"
      },
      "source": [
        "## Clean data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_bquvd8QUy4M"
      },
      "outputs": [],
      "source": [
        "# prompt: replace \\n with space in the df_toxic_data column less_toxic and more_toxic\n",
        "\n",
        "monty.df_toxic_data['less_toxic'] = monty.df_toxic_data['less_toxic'].str.replace('\\n', ' ')\n",
        "monty.df_toxic_data['more_toxic'] = monty.df_toxic_data['more_toxic'].str.replace('\\n', ' ')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Zfyt66nYkxy"
      },
      "outputs": [],
      "source": [
        "# prompt: replace \\n with space in the df_toxic_data column less_toxic and more_toxic\n",
        "\n",
        "monty.df_toxic_data['less_toxic'] = monty.df_toxic_data['less_toxic'].str.replace('http', 'tthp')\n",
        "monty.df_toxic_data['more_toxic'] = monty.df_toxic_data['more_toxic'].str.replace('http', 'tthp')\n",
        "monty.df_toxic_data['less_toxic'] = monty.df_toxic_data['less_toxic'].str.replace('.com', '.no')\n",
        "monty.df_toxic_data['more_toxic'] = monty.df_toxic_data['more_toxic'].str.replace('.com', '.no')\n",
        "monty.df_toxic_data['less_toxic'] = monty.df_toxic_data['less_toxic'].str.replace('.org', '.no')\n",
        "monty.df_toxic_data['more_toxic'] = monty.df_toxic_data['more_toxic'].str.replace('.org', '.no')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ep4Yw4idVrnO"
      },
      "outputs": [],
      "source": [
        "# prompt: replace any non-printing character with space in the df_toxic_data column less_toxic and more_toxic\n",
        "\n",
        "monty.df_toxic_data['less_toxic'] = monty.df_toxic_data['less_toxic'].str.replace('[^\\\\x00-\\\\x7F]', ' ')\n",
        "monty.df_toxic_data['more_toxic'] = monty.df_toxic_data['more_toxic'].str.replace('[^\\\\x00-\\\\x7F]', ' ')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oxs-rW7MVLnz"
      },
      "outputs": [],
      "source": [
        "# prompt: set panda row display to be 250 character long\n",
        "\n",
        "pandas.set_option('display.max_colwidth', 550)\n",
        "monty.df_toxic_data.sample(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HudGAJuLbJxR"
      },
      "outputs": [],
      "source": [
        "monty.df_toxic_data.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNmhXwKLvmS7"
      },
      "source": [
        "## Save it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E6LKY6-RuhN3"
      },
      "outputs": [],
      "source": [
        "# prompt: write monty.df_toxic_data dataframe to csv file\n",
        "\n",
        "monty.df_toxic_data.to_csv('toxic_data.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CUaMFR8KvCx8"
      },
      "outputs": [],
      "source": [
        "# %%write -a app.py\n",
        "\n",
        "# fname = 'toxic_data.csv'\n",
        "# monty.df_toxic_data = pandas.read_csv(fname)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCx5vYanSHsJ"
      },
      "source": [
        "# Investigate data\n",
        "\n",
        "- **PRIMARY ROLE:** Data engineer\n",
        "- Count average word size of more_toxic.\n",
        "- Count average word size of less_toxic.\n",
        "- Plot histogram\n",
        "- Report statistic\n",
        "- Draw word cloud"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-976yJ46w1RM"
      },
      "source": [
        "## Count words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XoFb2lqESJMN"
      },
      "outputs": [],
      "source": [
        "# prompt: create a new column in monty.df_toxic_data dataframe with the word count from \"less_toxic\" column\n",
        "\n",
        "monty.df_toxic_data['less_toxic_word_count'] = monty.df_toxic_data['less_toxic'].apply(lambda x: len(x.split()))\n",
        "monty.df_toxic_data.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3iJOe7l-S0N2"
      },
      "outputs": [],
      "source": [
        "# prompt: create a new column in monty.df_toxic_data dataframe with the word count from \"less_toxic\" column\n",
        "\n",
        "monty.df_toxic_data['more_toxic_word_count'] = monty.df_toxic_data['more_toxic'].apply(lambda x: len(x.split()))\n",
        "monty.df_toxic_data.sample(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZD2gZ2WS0Q2"
      },
      "outputs": [],
      "source": [
        "# prompt: find the sum of \"less_toxic_word_count\"\n",
        "\n",
        "lcount = monty.df_toxic_data['less_toxic_word_count'].sum()\n",
        "mcount = monty.df_toxic_data['more_toxic_word_count'].sum()\n",
        "print(lcount + mcount)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7va24DDxriU"
      },
      "source": [
        "## Draw histogram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AMYaxRFuT5KJ"
      },
      "outputs": [],
      "source": [
        "# prompt: using pandas to draw the histogram of \"less_toxic_word_count\"\n",
        "\n",
        "x = monty.df_toxic_data['less_toxic_word_count'].plot.hist(bins=10,\n",
        "  title='Less Toxic Word Count Histogram')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8fRmqDXT5N5"
      },
      "outputs": [],
      "source": [
        "x = monty.df_toxic_data['more_toxic_word_count'].plot.hist(bins=10,\n",
        "  title='More Toxic Word Count Histogram')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkwRj3vlyqXz"
      },
      "source": [
        "## Report statistic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6zQYhB7eS0T_"
      },
      "outputs": [],
      "source": [
        "# prompt: print the max, min, mean, and std of column \"les_toxic_word_count\"\n",
        "\n",
        "max_value = monty.df_toxic_data['less_toxic_word_count'].max()\n",
        "min_value = monty.df_toxic_data['less_toxic_word_count'].min()\n",
        "mean_value = monty.df_toxic_data['less_toxic_word_count'].mean()\n",
        "std_value = monty.df_toxic_data['less_toxic_word_count'].std()\n",
        "\n",
        "print(f\"Max: {max_value}, Min: {min_value}, Mean: {mean_value}, Std: {std_value}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qOfGf2w_SJSF"
      },
      "outputs": [],
      "source": [
        "# prompt: print the max, min, mean, and std of column \"les_toxic_word_count\"\n",
        "\n",
        "max_value = monty.df_toxic_data['more_toxic_word_count'].max()\n",
        "min_value = monty.df_toxic_data['more_toxic_word_count'].min()\n",
        "mean_value = monty.df_toxic_data['more_toxic_word_count'].mean()\n",
        "std_value = monty.df_toxic_data['more_toxic_word_count'].std()\n",
        "\n",
        "print(f\"Max: {max_value}, Min: {min_value}, Mean: {mean_value}, Std: {std_value}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DzsN-1s2Yt8F"
      },
      "outputs": [],
      "source": [
        "import wordcloud\n",
        "# help(wordcloud.WordCloud)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ql9iQZjuy_Hw"
      },
      "source": [
        "## Draw word cloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRvdUVTzSJU8"
      },
      "outputs": [],
      "source": [
        "# prompt: write a Python function with documentation for drawing a word cloud plot for a dataframe 'less_toxic_word_count'.\n",
        "\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def generate_wordcloud(df, column, title=\"Word Cloud\"):\n",
        "    \"\"\"\n",
        "    Generate a word cloud from text data in a specified DataFrame column.\n",
        "\n",
        "    Args:\n",
        "    df (pd.DataFrame): The DataFrame containing the text data.\n",
        "    column (str): The name of the column containing the text data.\n",
        "\n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "    # Ensure the column exists in the DataFrame\n",
        "    if column not in df.columns:\n",
        "        print(f\"The column {column} does not exist in the DataFrame.\")\n",
        "        return\n",
        "\n",
        "    # Combine all the text from the column into a single string\n",
        "    text = ' '.join(df[column].astype(str).values)\n",
        "\n",
        "    # create special word stops\n",
        "    my_stop_words = {'page', 'will', 'one', 'edit', 'article', 'know', 'way', 'say'}\n",
        "    combined_set = STOPWORDS.union(my_stop_words)\n",
        "\n",
        "    # Create a WordCloud object and generate the wordcloud\n",
        "    wordcloud = WordCloud(background_color='white', width=800, height=800,\n",
        "      max_words=300, stopwords=combined_set).generate(text)\n",
        "\n",
        "    # Display the generated wordcloud\n",
        "    plt.figure(figsize=(8,8),facecolor = None)\n",
        "    plt.imshow(wordcloud)\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(title)\n",
        "    plt.tight_layout(pad=0)\n",
        "    plt.show()\n",
        "\n",
        "    # Save the wordcloud to a file\n",
        "    wordcloud.to_file('wordcloud.png')\n",
        "    return\n",
        "\n",
        "# Example usage:-\n",
        "# generate_wordcloud(result_df, 'Article_Text')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7hL8K0uoW2IM"
      },
      "outputs": [],
      "source": [
        "generate_wordcloud(monty.df_toxic_data, \"less_toxic\", \"Less Toxic Word Cloud\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wubc9JvSW2LJ"
      },
      "outputs": [],
      "source": [
        "generate_wordcloud(monty.df_toxic_data, \"more_toxic\", \"More Toxic Word Cloud\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGDL-RMQW2YX"
      },
      "outputs": [],
      "source": [
        "# view word not count\n",
        "wordcloud.STOPWORDS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GE-02FKebQpL"
      },
      "outputs": [],
      "source": [
        "my_stop_words = {'page', 'will', 'one', 'edit', 'article', 'know', 'way', 'say'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RCxGAg8Zca2D"
      },
      "outputs": [],
      "source": [
        "# prompt: combine two sets, wordcloud.STOPWORDS and my_stop_words\n",
        "\n",
        "combined_set = wordcloud.STOPWORDS.union(my_stop_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "usiWUxYtca5R"
      },
      "outputs": [],
      "source": [
        "combined_set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cNBretL5cbZD"
      },
      "outputs": [],
      "source": [
        "# redraw them by re-run previous generate_wordcloud cell/command."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENILHFEQ3Qdq"
      },
      "source": [
        "# Reports\n",
        "\n",
        "- **PRIMARY ROLE:** AI QA engineer\n",
        "\n",
        "- Work with the Data engineer to write a report and potential data biases\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-TYGDL_AzG7"
      },
      "source": [
        "# Write API: Step 4\n",
        "\n",
        "- **PRIMARY ROLE:** AI scientist\n",
        "\n",
        "- Write a few functions to make the API and hook into Gradio and Huggingface."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UpXTPeD_IIFp"
      },
      "outputs": [],
      "source": [
        "# %%writefile -a app.py\n",
        "# prompt: (combine of many seperate prompts and copy code into one code cell)\n",
        "\n",
        "# for openai version 1.3.8\n",
        "@add_method(Pluto_Happy)\n",
        "#\n",
        "def _fetch_moderate_engine(self):\n",
        "  self.ai_client = openai.OpenAI(api_key=self._openai_key)\n",
        "  self.text_model = \"text-moderation-latest\"\n",
        "  return\n",
        "#\n",
        "@add_method(Pluto_Happy)\n",
        "# f\n",
        "def _censor_me(self, p, safer=0.0005):\n",
        "  self._fetch_moderate_engine()\n",
        "  resp_orig = self.ai_client.moderations.create(input=p, model=self.text_model)\n",
        "  resp_dict = resp_orig.model_dump()\n",
        "  #\n",
        "  v1 = resp_dict[\"results\"][0][\"category_scores\"]\n",
        "  v1 = {key: value if value is not None else 0 for key, value in v1.items()}\n",
        "  print(f'resp_dic: {resp_dict}')\n",
        "  print(f'v1: {v1}')\n",
        "\n",
        "  max_key = max(v1, key=v1.get)\n",
        "  max_value = v1[max_key]\n",
        "  sum_value = sum(v1.values())\n",
        "  #\n",
        "  v1[\"is_safer_flagged\"] = False\n",
        "  if (max_value >= safer):\n",
        "    v1[\"is_safer_flagged\"] = True\n",
        "  v1[\"is_flagged\"] = resp_dict[\"results\"][0][\"flagged\"]\n",
        "  v1['max_key'] = max_key\n",
        "  v1['max_value'] = max_value\n",
        "  v1['sum_value'] = sum_value\n",
        "  v1['safer_value'] = safer\n",
        "  v1['message'] = p\n",
        "  return v1\n",
        "#\n",
        "@add_method(Pluto_Happy)\n",
        "def _draw_censor(self,data):\n",
        "  self._color_mid_gray = '#6c757d'\n",
        "  exp = (0.01, 0.01)\n",
        "  x = [data['max_value'], (1-data['max_value'])]\n",
        "  title=f\"\\nUnsafe: {data['max_key']}: {(data['max_value']*100):.2f}% Confidence\\n\"\n",
        "  lab = [data['max_key'], 'Other 13 categories']\n",
        "  if (data['is_flagged']):\n",
        "    col=[self.color_danger, self.color_mid_gray]\n",
        "  elif (data['is_safer_flagged']):\n",
        "    col=[self.color_warning, self.color_mid_gray]\n",
        "    lab = ['Relative Score:\\n'+data['max_key'], 'Other 13 categories']\n",
        "    title=f\"\\nPersonal Unsafe: {data['max_key']}: {(data['max_value']*100):.2f}% Confidence\\n\"\n",
        "  else:\n",
        "    col=[self.color_mid_gray, self.color_success]\n",
        "    lab = ['False Negative:\\n'+data['max_key'], 'Other 13 categories']\n",
        "    title='\\nSafe Message\\n'\n",
        "  canvas = self._draw_donut(x, lab, col, exp,title)\n",
        "  return canvas\n",
        "#\n",
        "@add_method(Pluto_Happy)\n",
        "def _draw_donut(self,data,labels,col, exp,title):\n",
        "  # col = [self.color_danger, self._color_secondary]\n",
        "  # exp = (0.01, 0.01)\n",
        "  # Create a pie chart\n",
        "  canvas, pic = matplotlib.pyplot.subplots()\n",
        "  pic.pie(data, explode=exp,\n",
        "    labels=labels,\n",
        "    colors=col,\n",
        "    autopct='%1.1f%%',\n",
        "    startangle=90,\n",
        "    textprops={'color':'#0a0a0a'})\n",
        "  # Draw a circle at the center of pie to make it look like a donut\n",
        "  # centre_circle = matplotlib.pyplot.Circle((0,0),0.45,fc='white')\n",
        "  centre_circle = matplotlib.pyplot.Circle((0,0),0.45,fc=col[0],linewidth=2, ec='white')\n",
        "  canvas = matplotlib.pyplot.gcf()\n",
        "  canvas.gca().add_artist(centre_circle)\n",
        "\n",
        "  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
        "  pic.axis('equal')\n",
        "  pic.set_title(title)\n",
        "  canvas.tight_layout()\n",
        "  # canvas.show()\n",
        "  return canvas\n",
        "#\n",
        "@add_method(Pluto_Happy)\n",
        "# def censor_me(self, msg, safer=0.02, ibutton_1=0):\n",
        "def fetch_toxicity_level(self, msg, safer):\n",
        "  # safer=0.2\n",
        "  yjson = self._censor_me(msg,safer)\n",
        "  _canvas = self._draw_censor(yjson)\n",
        "  _yjson = json.dumps(yjson, indent=4)\n",
        "  return (_canvas, _yjson)\n",
        "  #return(_canvas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50UWtl4KPhCA"
      },
      "outputs": [],
      "source": [
        "# help(matplotlib.pyplot.Circle)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JC2DivZF4grS"
      },
      "source": [
        "## Smoke test the functions\n",
        "\n",
        "- **PRIMARY ROLE:** AI QA engineer\n",
        "\n",
        "- QA all the functions above"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "djysBd841sAa"
      },
      "outputs": [],
      "source": [
        "# prompt: none\n",
        "# Smoke test\n",
        "resp = monty._censor_me(\"I am but a sheep who is lost in the wood.\")\n",
        "resp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9wbfl2Uft3f9"
      },
      "outputs": [],
      "source": [
        "donut, jreturn = monty.fetch_toxicity_level(\"I am but a sheep who is lost in the wood.\", 0.02)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Omu7T24nuGAr"
      },
      "outputs": [],
      "source": [
        "print(jreturn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ABOp6bgXcTSy"
      },
      "outputs": [],
      "source": [
        "# # prompt: print the first value of monty.df_toxic_data['more_toxic']\n",
        "\n",
        "# monty.df_toxic_data['more_toxic'].values[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DFtR4kkHa041"
      },
      "outputs": [],
      "source": [
        "# prompt: none\n",
        "\n",
        "# test with kaggle data\n",
        "msg = str(monty.df_toxic_data['more_toxic'].sample(1).values[0])\n",
        "resp = monty._censor_me(msg)\n",
        "resp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LIGrnIDJ2s7w"
      },
      "outputs": [],
      "source": [
        "# prompt: none\n",
        "\n",
        "# again: test with kaggle data\n",
        "msg = str(monty.df_toxic_data['more_toxic'].sample(1).values[0])\n",
        "resp = monty._censor_me(msg)\n",
        "resp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IqhAH3Ok2xjq"
      },
      "outputs": [],
      "source": [
        "# prompt: none\n",
        "\n",
        "# again: test with kaggle data\n",
        "msg = str(monty.df_toxic_data['more_toxic'].sample(1).values[0])\n",
        "resp = monty._censor_me(msg)\n",
        "resp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqiP3ho3JbKW"
      },
      "source": [
        "# Define HuggingFace Gradio Interface\n",
        "\n",
        "- PRIMARY ROLE: AI scientist\n",
        "\n",
        "- Build from scratch using Blocks() instead of the short cut using Interface()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R9XhrCVWuP7d"
      },
      "outputs": [],
      "source": [
        "# %%write -a app.py\n",
        "# prompt: result from a lot of prompt AI and old fashion try and error\n",
        "\n",
        "import random\n",
        "def say_hello(val):\n",
        "  return f\"Hello: {val}\"\n",
        "def say_toxic():\n",
        "  return f\"I am toxic\"\n",
        "def fetch_toxic_tweets(maxi=2):\n",
        "    sample_df = monty.df_toxic_data.sample(maxi)\n",
        "    is_true = random.choice([True, False])\n",
        "    c1 = \"more_toxic\"\n",
        "    if is_true:\n",
        "      c1 = \"less_toxic\"\n",
        "    toxic1 = sample_df[c1].iloc[0]\n",
        "    # toxic1 = \"cat eats my homework.\"\n",
        "    return sample_df.to_html(index=False), toxic1\n",
        "#\n",
        "# define all gradio widget/components outside the block for easy to visualize the blocks structure\n",
        "#\n",
        "in1 = gradio.Textbox(lines=3, label=\"Enter Text:\")\n",
        "in2 = gradio.Slider(0.005, .1, value=0.02, step=.005,label=\"Personalize Safer Value: (larger value is less safe)\")\n",
        "out1 = gradio.Plot(label=\"Output:\")\n",
        "out2 = gradio.HTML(label=\"Real-world Toxic Posts/Tweets: *WARNING\")\n",
        "out3 = gradio.Textbox(lines=5, label=\"Output JSON:\")\n",
        "but1 = gradio.Button(\"Measure 14 Toxicity\", variant=\"primary\",size=\"sm\")\n",
        "but2 = gradio.Button(\"Fetch Toxic Text\", variant=\"stop\", size=\"sm\")\n",
        "#\n",
        "txt1 = \"\"\"\n",
        "# Welcome To The Friendly Text Moderation\n",
        "\n",
        "### Identify 14 categories of text toxicity.\n",
        "\n",
        "> This NLP (Natural Language Processing) AI demonstration aims to prevent profanity, vulgarity, hate speech, violence, sexism, and other offensive language.\n",
        ">It is **not an act of censorship**, as the final UI (User Interface) will give the reader, but not a young reader, the option to click on a label to read the toxic message.\n",
        ">The goal is to create a safer and more respectful environment for you, your colleages, and your family.\n",
        "> This NLP app is 1 of 3 hands-on courses, [\"AI Solution Architect,\" from ELVTR and Duc Haba](https://elvtr.com/course/ai-solution-architect?utm_source=instructor&utm_campaign=AISA&utm_content=linkedin).\n",
        "---\n",
        "### Helpful Instruction:\n",
        "\n",
        "1. Enter your [harmful] message in the input box.\n",
        "\n",
        "2. Click the \"Measure 14 Toxicity\" button.\n",
        "3. View the result on the Donut plot.\n",
        "4. (**Optional**) Click on the \"Fetch Real World Toxic Dataset\" below.\n",
        "5. There are additional options and notes below.\n",
        "\"\"\"\n",
        "txt2 = \"\"\"\n",
        "## Author and Developer Notes:\n",
        "---\n",
        "- The demo uses the cutting-edge (2024) AI Natural Language Processing (NLP) model from OpenAI.\n",
        "- This NLP app is 1 of 3 hands-on apps from the [\"AI Solution Architect,\" from ELVTR and Duc Haba](https://elvtr.com/course/ai-solution-architect?utm_source=instructor&utm_campaign=AISA&utm_content=linkedin).\n",
        "\n",
        "- It is not a Generative (GenAI) model, such as Google Gemini or GPT-4.\n",
        "- The NLP understands the message context, nuance, innuendo, and not just swear words.\n",
        "- We **challenge you** to trick it, i.e., write a toxic tweet or post, but our AI thinks it is safe. If you win, please send us your message.\n",
        "- The 14 toxicity categories are as follows:\n",
        "\n",
        "    1. harassment\n",
        "    2. harassment threatening\n",
        "    3. harassment instructions\n",
        "    4. hate\n",
        "    5. hate threatening\n",
        "    6. hate instructions\n",
        "    7. self harm\n",
        "    8. self harm instructions\n",
        "    9. self harm intent\n",
        "    10. self harm minor\n",
        "    11. sexual\n",
        "    12. sexual minors\n",
        "    13. violence\n",
        "    14. violence graphic\n",
        "\n",
        "- If the NLP model classifies the message as \"safe,\" you can still limit the level of toxicity by using the \"Personal Safe\" slider.\n",
        "- The smaller the personal-safe value, the stricter the limitation. It means that if you're a young or sensitive adult, you should choose a lower personal-safe value, less than 0.02, to ensure you're not exposed to harmful content.\n",
        "- The color of the donut plot is as follows:\n",
        "  - Red is an \"unsafe\" message by the NLP model\n",
        "  - Green is a \"safe\" message\n",
        "  - Yellow is an \"unsafe\" message by your toxicity level\n",
        "\n",
        "- The **\"confidence\"** score refers to the confidence level in detecting a particular type of toxicity among the 14 tracked types. For instance, if the confidence score is 90%, it indicates a 90% chance that the toxicity detected is of that particular type. In comparison, the remaining 13 toxicities collectively have a 10% chance of being the detected toxicity. Conversely, if the confidence score is 3%, it could indicate any toxicity. It's worth noting that the Red, Green, or Yellow safety levels do not influence the confidence score.\n",
        "\n",
        "- The real-world dataset is from the Jigsaw Rate Severity of Toxic Comments on Kaggle. It has 30,108 records.\n",
        "    - Citation:\n",
        "    - Ian Kivlichan, Jeffrey Sorensen, Lucas Dixon, Lucy Vasserman, Meghan Graham, Tin Acosta, Walter Reade. (2021). Jigsaw Rate Severity of Toxic Comments . Kaggle. https://kaggle.com/competitions/jigsaw-toxic-severity-rating\n",
        "- The intent is to share with Duc's friends and colleagues, but for those with nefarious intent, this Text Moderation model is governed by the GNU 3.0 License: https://www.gnu.org/licenses/gpl-3.0.en.html\n",
        "- Author: Copyright (C), 2024 **[Duc Haba](https://linkedin.com/in/duchaba)**\n",
        "---\n",
        "# \"AI Solution Architect\" Course by ELVTR\n",
        "\n",
        ">Welcome to the fascinating world of AI and natural language processing (NLP). This NLP model is a part of one of three hands-on application. In our journey together, we will explore the [AI Solution Architect](https://elvtr.com/course/ai-solution-architect?utm_source=instructor&utm_campaign=AISA&utm_content=linkedin) course, meticulously crafted by ELVTR in collaboration with Duc Haba. This course is intended to serve as your gateway into the dynamic and constantly evolving field of AI Solution Architect, providing you with a comprehensive understanding of its complexities and applications.\n",
        "\n",
        ">An AI Solution Architect (AISA) is a mastermind who possesses a deep understanding of the complex technicalities of AI and knows how to creatively integrate them into real-world solutions. They bridge the gap between theoretical AI models and practical, effective applications. AISA works as a strategist to design AI systems that align with business objectives and technical requirements. They delve into algorithms, data structures, and computational theories to translate them into tangible, impactful AI solutions that have the potential to revolutionize industries.\n",
        "\n",
        "> [Sign up for the course today](https://elvtr.com/course/ai-solution-architect?utm_source=instructor&utm_campaign=AISA&utm_content=linkedin), and I will see you in class.\n",
        "\n",
        "- An article about this NLP Text Moderation will be coming soon.\n",
        "\"\"\"\n",
        "txt3 = \"\"\"\n",
        "## WARNING: WARNING:\n",
        "---\n",
        "\n",
        "- The following button will retrieve **real-world** offensive posts from Twitter and customer reviews from consumer companies.\n",
        "- The button will display four toxic messages at a time. **Click again** for four more randomly selected postings/tweets.\n",
        "- They contain **profanity, vulgarity, hate, violence, sexism, and other offensive language.**\n",
        "- After you fetch the toxic messages, Click on the **\"Measure 14 Toxicity\" button**.\n",
        "\"\"\"\n",
        "#reverse_button.click(process_text, inputs=text_input, outputs=reversed_text)\n",
        "#\n",
        "\n",
        "with gradio.Blocks() as gradio_app:\n",
        "  # title\n",
        " gradioMarkdown(txt1) # any html or simple mark up\n",
        "  #\n",
        "  # first row, has two columns 1/3 size and 2/3 size\n",
        "  with gradio.Row():    # items inside rows are columns\n",
        "    # left column\n",
        "    with gradio.Column(scale=1): # items under columns are row, scale is 1/3 size\n",
        "      # left column has two rows, text entry, and buttons\n",
        "      in1.render()\n",
        "      in2.render()\n",
        "      but1.render()\n",
        "      out3.render()\n",
        "      but1.click(monty.fetch_toxicity_level, inputs=[in1, in2], outputs=[out1,out3])\n",
        "\n",
        "    with gradio.Column(scale=2):\n",
        "      out1.render()\n",
        "  #\n",
        "  # second row is warning text\n",
        "  with gradio.Row():\n",
        "    gradio.Markdown(txt3)\n",
        "\n",
        "  # third row is fetching toxic data\n",
        "  with gradio.Row():\n",
        "    with gradio.Column(scale=1):\n",
        "      but2.render()\n",
        "      but2.click(fetch_toxic_tweets, inputs=None, outputs=[out2, in1])\n",
        "    with gradio.Column(scale=2):\n",
        "      out2.render()\n",
        "\n",
        "  # fourth row is note text\n",
        "  with gradio.Row():\n",
        "    gradio.Markdown(txt2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzCOApzwIC7V"
      },
      "source": [
        "# QA - Test it locally on Jupyter Notebook: STEP 5\n",
        "\n",
        "- It will failed to test locally if you running VPN\n",
        "\n",
        "- You should see the app run locally/here as if it is on HuggingFace.\n",
        " - https://huggingface.co/spaces/duchaba/Friendly_Text_Moderation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Adttj_MSuP_y"
      },
      "outputs": [],
      "source": [
        "# %%write -a app.py\n",
        "# prompt: start graido_app\n",
        "\n",
        "#gradio_app.launch(debug=True)\n",
        "gradio_app.launch()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwC_rWcNPkVn"
      },
      "source": [
        "# Presentation and Review\n",
        "\n",
        "- **PRIMARY ROLE:** AI solution architect\n",
        "- As an AI solution architect, you will review the work above.\n",
        "\n",
        "- The rest of the steps are for DevOps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YHQsNl05P21"
      },
      "source": [
        "# Production Deployment\n",
        "\n",
        "- *Duc: I have not clean up this section yet.*\n",
        "\n",
        "- **PRIMARY ROLE:** DevOps engineer\n",
        "\n",
        "- We choose Huggingface website, but in realword it would on AWS or Google Serverless engine or MS Azure servers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anKLQ3g8vlZm"
      },
      "source": [
        "## Write/create required files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SDo1cXLVzVNF"
      },
      "outputs": [],
      "source": [
        "@add_method(Pluto_Happy)\n",
        "def fetch_code_cells(self, notebook_name,\n",
        "  filter_magic=\"# %%write\",\n",
        "  write_to_file=True, fname_override=None):\n",
        "\n",
        "  \"\"\"\n",
        "  Reads a Jupyter notebook (.ipynb file) and writes out all the code cells\n",
        "  that start with the specified magic command to a .py file.\n",
        "\n",
        "  Parameters:\n",
        "  - notebook_name (str): Name of the notebook file (with .ipynb extension).\n",
        "  - filter_magic (str): Magic command filter. Only cells starting with this command will be written.\n",
        "      The defualt is: \"# %%write\"\n",
        "  - write_to_file (bool): If True, writes the filtered cells to a .py file.\n",
        "      Otherwise, prints them to the standard output. The default is True.\n",
        "  - fname_override (str): If provided, overrides the output filename. The default is None.\n",
        "\n",
        "  Returns:\n",
        "  - None: Writes the filtered code cells to a .py file or prints them based on the parameters.\n",
        "\n",
        "  \"\"\"\n",
        "  with open(notebook_name, 'r', encoding='utf-8') as f:\n",
        "    notebook_content = json.load(f)\n",
        "\n",
        "  output_content = []\n",
        "\n",
        "  # Loop through all the cells in the notebook\n",
        "  for cell in notebook_content['cells']:\n",
        "    # Check if the cell type is 'code' and starts with the specified magic command\n",
        "    if cell['cell_type'] == 'code' and cell['source'] and cell['source'][0].startswith(filter_magic):\n",
        "      # Append the source code of the cell to output_content\n",
        "      output_content.append(''.join(cell['source']))\n",
        "\n",
        "  if write_to_file:\n",
        "    if fname_override is None:\n",
        "      # Derive the output filename by replacing .ipynb with .py\n",
        "      output_filename = notebook_name.replace(\".ipynb\", \".py\")\n",
        "    else:\n",
        "      output_filename = fname_override\n",
        "    with open(output_filename, 'w', encoding='utf-8') as f:\n",
        "      f.write('\\n'.join(output_content))\n",
        "    print(f'File: {output_filename} written to disk.')\n",
        "  else:\n",
        "    # Print the code cells to the standard output\n",
        "    print('\\n'.join(output_content))\n",
        "    print('-' * 40)  # print separator\n",
        "  return\n",
        "# Example usage:\n",
        "# print_code_cells_from_notebook('your_notebook_name_here.ipynb')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fE3_ezCcKNjR"
      },
      "outputs": [],
      "source": [
        "# define the huggingface name\n",
        "monty.hface_space = 'duchaba/Friendly_Text_Moderation'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xn_RtPBoJnIG"
      },
      "outputs": [],
      "source": [
        "# openai: 0.27.7,              Actual: 0.27.7\n",
        "# huggingface_hub: 0.14.1,     Actual: 0.15.1\n",
        "# gradio: 3.32.0,              Actual: 3.32.0\n",
        "# cryptography: 40.0.2,        Actual: 40.0.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dh_AKuSj0tq_"
      },
      "outputs": [],
      "source": [
        "!cat \"/content/pluto_happy/requirements.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N6r8821ktW9Y"
      },
      "outputs": [],
      "source": [
        "# create the requirements.txt file\n",
        "txt = [\"openai\", \"gradio\",\"cryptography\", \"huggingface_hub\", \"psutil\", \"pynvml\", \"py-cpuinfo\", \"flopth\"]\n",
        "monty.write_file(\"requirements.txt\", txt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pSm8cahGvFxI"
      },
      "outputs": [],
      "source": [
        "# optional double check it\n",
        "!cat requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KjPwdG6X1Gz"
      },
      "source": [
        "**STOP**\n",
        "\n",
        "1. Download this notebook\n",
        "\n",
        "1. Upload it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XodnoxcL1m-9"
      },
      "outputs": [],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C1yKXHVnXCh7"
      },
      "outputs": [],
      "source": [
        "fname = \"/content/AISA_Text_Moderation.ipynb\"\n",
        "monty.fetch_code_cells(fname, fname_override=\"app_part2.py\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_x-1VYWK2jRD"
      },
      "outputs": [],
      "source": [
        "# prompt: use unix command to concat file1 and file2\n",
        "\n",
        "!cat /content/pluto_happy/pluto.py app_part2.py > app.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HDS8KtUEtXAo"
      },
      "outputs": [],
      "source": [
        "# uncomment the %%write code cell about to create app.py\n",
        "# then double check it\n",
        "!cat app.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsF4xLdREBx8"
      },
      "source": [
        "## Create the HuggingFace page\n",
        "\n",
        "- Choose a unique file-space, like happy_butterfly\n",
        "\n",
        "- First option, do it on huggingface.com website (recomented)\n",
        "\n",
        "- Second option, do it programatically (optional, uncomment below)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dulzFow1AT68"
      },
      "outputs": [],
      "source": [
        "# # second option\n",
        "# api = huggingface_hub.HfApi()\n",
        "# api.create_repo(repo_id=pluto.hface_name, private=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKF1nWC3MRJO"
      },
      "source": [
        "# Deploy to HuggingFace Sandbox\n",
        "\n",
        "---\n",
        "\n",
        "- Read the tutorial above if you are confused.\n",
        "\n",
        "- It is easy. \"app.py\" and \"requirements.txt\" are the two files that you need to upload.\n",
        "  - Link to create the app.py file on huggingface web: https://huggingface.co/spaces/duchaba/new/main?filename=app.py\n",
        "\n",
        "- Optional are depending on your more fancy layout/output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPQFNwRXv80u"
      },
      "source": [
        "## Push to files to Hugging face"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kHfqwn6h7Bgl"
      },
      "outputs": [],
      "source": [
        "@add_method(Pluto_Happy)\n",
        "def _login_hface(self):\n",
        "  huggingface_hub.login(self._huggingface_key,\n",
        "    add_to_git_credential=True) # non-blocking login\n",
        "  self._ph()\n",
        "  return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mGWpv6Sc46Tn"
      },
      "outputs": [],
      "source": [
        "monty._login_hface()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Qc9s4jO7V7V"
      },
      "outputs": [],
      "source": [
        "monty.hface_space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yWkJsiH9AT-N"
      },
      "outputs": [],
      "source": [
        "up_files = [\"app.py\", \"requirements.txt\", \"toxic_data.csv\"]\n",
        "monty.push_hface_files(up_files, hf_space=monty.hface_space)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tXzSFXj-0reI"
      },
      "outputs": [],
      "source": [
        "# test it\n",
        "print(f'https://huggingface.co/spaces/{monty.hface_space}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0BqlE3dGbnq"
      },
      "source": [
        "# Pull and Push to Github (Optional)\n",
        "\n",
        "- **PRIMARY ROLE:** DevOps engineer\n",
        "\n",
        "- *Duc: I have not clean up this section yet.*\n",
        "\n",
        "**Note:**\n",
        "\n",
        "- QA it on this notebook **BEFORE** push it.\n",
        "\n",
        "- If you change any data or files, commit and push it to github. For now, we don't need pull-request, so push it to main or your-branch-name.\n",
        "\n",
        "- I ussualy comment out the section because I don't want to accidental run it (when not ready)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MIVSHSwmGbnq"
      },
      "outputs": [],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kxPpK4xmGKDX"
      },
      "outputs": [],
      "source": [
        "fname = 'https://github.com/AISA-DucHaba/AI-Solution-Architect.git'\n",
        "!git clone {fname}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iytF4it5eCt3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "f = '/content/AI-Solution-Architect'\n",
        "os.chdir(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X-nh-RICGpaj"
      },
      "outputs": [],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lBESQ2AAGbnq"
      },
      "outputs": [],
      "source": [
        "!git branch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FlvaEHrEGbnr"
      },
      "outputs": [],
      "source": [
        "!git pull"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jKnWWmInGbnr"
      },
      "outputs": [],
      "source": [
        "# check for update file\n",
        "!git diff --name-only"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Nonybh2NGP_"
      },
      "outputs": [],
      "source": [
        "# prompt: show the github diff in previous version\n",
        "\n",
        "!git diff HEAD^ HEAD\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uWTzuEkld8E6"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# f = 'Data-Augmentation-with-Python'\n",
        "# os.chdir(f)\n",
        "!git add -A\n",
        "!git config --global user.email \"duc.haba@gmail.com\"\n",
        "!git config --global user.name \"duchaba\"\n",
        "!git commit -m \"add raw data files from kaggle to data directory\"\n",
        "# # do the git push in the xterm console\n",
        "# #!git push"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMDuDF5gGbnr"
      },
      "outputs": [],
      "source": [
        "# # check for any in stage ready to commit\n",
        "# !git diff --name-only --staged"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pAIl7ZzEGbns"
      },
      "outputs": [],
      "source": [
        "# check what were the commits\n",
        "!git log --name-status HEAD^..HEAD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Azo-O8pGbnt"
      },
      "outputs": [],
      "source": [
        "# double check it before commit\n",
        "!git status"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dmPsmXZBuaB3"
      },
      "outputs": [],
      "source": [
        "# !pip install lfs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fU6cc4f4ulGZ"
      },
      "outputs": [],
      "source": [
        "# prompt: git push large file\n",
        "\n",
        "# !git-lfs track *.pkl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dWaoO8u2Gbnt"
      },
      "outputs": [],
      "source": [
        "# push it\n",
        "fname = 'https://duchaba:@github.com/AISA-DucHaba/AI-Solution-Architect.git'\n",
        "!git push {fname}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NDRDuZL6Mnvt"
      },
      "outputs": [],
      "source": [
        "# !curl https://api.openai.com/v1/moderations \\\n",
        "#   -H \"Content-Type: application/json\" \\\n",
        "#   -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n",
        "#   -d '{\"input\": \"I want to kill them all.\"}'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9wsZtfaqTYAN"
      },
      "outputs": [],
      "source": [
        "# prompt: zip a directory data\n",
        "fname = '/content/AI-Solution-Architect/data'\n",
        "!zip -r data.zip {fname}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oDkCp0WxTpQK"
      },
      "outputs": [],
      "source": [
        "# prompt: download /content/AI-Solution-Architect/data.zip file\n",
        "\n",
        "from google.colab import files\n",
        "files.download('/content/AI-Solution-Architect/data.zip')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_jLD0Z7AUKk"
      },
      "source": [
        "# That's it. It's dancing time:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pUzE7MfOI1Sl"
      },
      "outputs": [],
      "source": [
        "print('the end.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9VL5egEhI16n"
      },
      "outputs": [],
      "source": [
        "# monty.print_dancing()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2x-NQlhNSD9"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "- That is it for the NLP text moderation from soup to nuts.\n",
        "\n",
        "- Use this LLM \"as-is\" or as a template to create your own model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6vkge-9YX18b"
      },
      "outputs": [],
      "source": [
        "v1 = {'harassment': 0.4044782519340515, 'harassment_threatening': 0.2582634687423706, 'hate': 0.00500369630753994, 'hate_threatening': 0.0005717452731914818, 'illicit': None, 'illicit_violent': None, 'self_harm': 0.0004080208018422127, 'self_harm_instructions': 1.6580557712586597e-05, 'self_harm_intent': 0.00043149717384949327, 'sexual': 0.0015329490415751934, 'sexual_minors': 6.829887570347637e-05, 'violence': 0.816072404384613, 'violence_graphic': 0.0012203836813569069, 'self-harm': 0.0004080208018422127, 'sexual/minors': 6.829887570347637e-05, 'hate/threatening': 0.0005717452731914818, 'violence/graphic': 0.0012203836813569069, 'self-harm/intent': 0.00043149717384949327, 'self-harm/instructions': 1.6580557712586597e-05, 'harassment/threatening': 0.2582634687423706}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NIs9gqg_ZCrb"
      },
      "outputs": [],
      "source": [
        "v2 = {key: value if value is not None else 0 for key, value in v1.items()}\n",
        "v2.values()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fw-_zG0ZX_uV"
      },
      "outputs": [],
      "source": [
        "sum_value = sum(value for value in v1.values() if value is not None)\n",
        "sum_value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pgv9ExwDX_xh"
      },
      "outputs": [],
      "source": [
        "v1.values()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyPXTmcBA4wfNy5NWIxTKCon",
      "gpuType": "T4",
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
